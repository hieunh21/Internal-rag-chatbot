"""
Incremental Index Update Script

Features:
    - Detect files m·ªõi/thay ƒë·ªïi/x√≥a
    - Ch·ªâ update ph·∫ßn c·∫ßn thi·∫øt (kh√¥ng rebuild to√†n b·ªô)
    - Backup index c≈© tr∆∞·ªõc khi update
    - Rollback n·∫øu c√≥ l·ªói
    
Usage:
    python scripts/update_index.py              # Check v√† update
    python scripts/update_index.py --force      # Force rebuild to√†n b·ªô
    python scripts/update_index.py --dry-run    # Ch·ªâ check, kh√¥ng update
"""

import os
import sys
import json
import hashlib
import shutil
import argparse
from datetime import datetime
from typing import Dict, List, Set, Tuple

# Th√™m parent directory v√†o path ƒë·ªÉ import app modules
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document

from app.ingest import LocalEmbedding
from app.config import settings


# ================================================================
# CONSTANTS
# ================================================================

MANIFEST_FILE = os.path.join(settings.DB_DIR, "manifest.json")
BACKUP_DIR = os.path.join(settings.DB_DIR, "backup")


# ================================================================
# HELPER FUNCTIONS
# ================================================================

def calculate_file_hash(filepath: str) -> str:
    """T√≠nh MD5 hash c·ªßa file ƒë·ªÉ detect thay ƒë·ªïi"""
    hash_md5 = hashlib.md5()
    with open(filepath, "rb") as f:
        for chunk in iter(lambda: f.read(4096), b""):
            hash_md5.update(chunk)
    return hash_md5.hexdigest()


def get_current_files(data_dir: str) -> Dict[str, str]:
    """
    Scan th∆∞ m·ª•c v√† tr·∫£ v·ªÅ dict {filename: hash}
    """
    files = {}
    for filename in os.listdir(data_dir):
        if filename.endswith(".md"):
            filepath = os.path.join(data_dir, filename)
            files[filename] = calculate_file_hash(filepath)
    return files


def load_manifest() -> Dict:
    """Load manifest t·ª´ file"""
    if os.path.exists(MANIFEST_FILE):
        with open(MANIFEST_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    return {"files": {}, "last_update": None, "version": 1}


def save_manifest(manifest: Dict) -> None:
    """L∆∞u manifest ra file"""
    manifest["last_update"] = datetime.now().isoformat()
    with open(MANIFEST_FILE, "w", encoding="utf-8") as f:
        json.dump(manifest, f, indent=2, ensure_ascii=False)


def detect_changes(
    current_files: Dict[str, str],
    manifest: Dict
) -> Tuple[Set[str], Set[str], Set[str]]:
    """
    So s√°nh files hi·ªán t·∫°i v·ªõi manifest
    
    Returns:
        (new_files, modified_files, deleted_files)
    """
    old_files = manifest.get("files", {})
    
    current_set = set(current_files.keys())
    old_set = set(old_files.keys())
    
    # Files m·ªõi (c√≥ trong current, kh√¥ng c√≥ trong old)
    new_files = current_set - old_set
    
    # Files ƒë√£ x√≥a (c√≥ trong old, kh√¥ng c√≥ trong current)
    deleted_files = old_set - current_set
    
    # Files ƒë√£ s·ª≠a (hash kh√°c)
    modified_files = set()
    for filename in current_set & old_set:
        if current_files[filename] != old_files[filename]:
            modified_files.add(filename)
    
    return new_files, modified_files, deleted_files


# ================================================================
# DOCUMENT PROCESSING
# ================================================================

def load_and_split_file(filepath: str) -> List[Document]:
    """Load v√† split m·ªôt file th√†nh chunks"""
    with open(filepath, "r", encoding="utf-8") as f:
        content = f.read()
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=settings.CHUNK_SIZE,
        chunk_overlap=settings.CHUNK_OVERLAP,
        separators=["\n## ", "\n### ", "\n#### ", "\n\n", "\n", " "]
    )
    
    chunks = splitter.split_text(content)
    
    documents = []
    for i, chunk in enumerate(chunks):
        doc = Document(
            page_content=chunk,
            metadata={
                "source": filepath,
                "chunk_id": i,
                "filename": os.path.basename(filepath)
            }
        )
        documents.append(doc)
    
    return documents


# ================================================================
# INDEX OPERATIONS
# ================================================================

def backup_index() -> bool:
    """Backup index hi·ªán t·∫°i"""
    if not os.path.exists(settings.DB_DIR):
        return False
    
    # T·∫°o backup directory
    os.makedirs(BACKUP_DIR, exist_ok=True)
    
    # Copy files
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    backup_path = os.path.join(BACKUP_DIR, timestamp)
    os.makedirs(backup_path, exist_ok=True)
    
    for filename in ["index.faiss", "index.pkl", "manifest.json"]:
        src = os.path.join(settings.DB_DIR, filename)
        if os.path.exists(src):
            shutil.copy2(src, backup_path)
    
    print(f"  ‚úì Backed up to: {backup_path}")
    return True


def restore_from_backup(backup_path: str) -> bool:
    """Restore index t·ª´ backup"""
    if not os.path.exists(backup_path):
        return False
    
    for filename in ["index.faiss", "index.pkl", "manifest.json"]:
        src = os.path.join(backup_path, filename)
        if os.path.exists(src):
            shutil.copy2(src, settings.DB_DIR)
    
    print(f"  ‚úì Restored from: {backup_path}")
    return True


def rebuild_full_index(data_dir: str, embeddings) -> FAISS:
    """Rebuild to√†n b·ªô index t·ª´ ƒë·∫ßu"""
    print("\nüî® Rebuilding full index...")
    
    all_documents = []
    
    for filename in os.listdir(data_dir):
        if filename.endswith(".md"):
            filepath = os.path.join(data_dir, filename)
            docs = load_and_split_file(filepath)
            all_documents.extend(docs)
            print(f"  ‚úì {filename}: {len(docs)} chunks")
    
    print(f"\nüìä Total: {len(all_documents)} chunks")
    
    # T·∫°o FAISS index
    print("  Creating FAISS index...")
    db = FAISS.from_documents(all_documents, embeddings)
    
    return db


def incremental_update(
    db: FAISS,
    new_files: Set[str],
    modified_files: Set[str],
    deleted_files: Set[str],
    data_dir: str,
    embeddings
) -> FAISS:
    """
    Update index m·ªôt c√°ch incremental
    
    Note: FAISS kh√¥ng h·ªó tr·ª£ delete documents tr·ª±c ti·∫øp,
    n√™n v·ªõi modified/deleted files, ta c·∫ßn rebuild.
    """
    # N·∫øu c√≥ files b·ªã x√≥a ho·∫∑c s·ª≠a, c·∫ßn rebuild
    if modified_files or deleted_files:
        print("\n‚ö†Ô∏è Modified/deleted files detected - need full rebuild")
        print(f"   Modified: {modified_files}")
        print(f"   Deleted: {deleted_files}")
        return rebuild_full_index(data_dir, embeddings)
    
    # N·∫øu ch·ªâ c√≥ files m·ªõi, c√≥ th·ªÉ add th√™m
    if new_files:
        print(f"\n‚ûï Adding {len(new_files)} new files...")
        
        new_documents = []
        for filename in new_files:
            filepath = os.path.join(data_dir, filename)
            docs = load_and_split_file(filepath)
            new_documents.extend(docs)
            print(f"  ‚úì {filename}: {len(docs)} chunks")
        
        print(f"  Adding {len(new_documents)} new chunks to index...")
        db.add_documents(new_documents)
        
        return db
    
    # Kh√¥ng c√≥ g√¨ thay ƒë·ªïi
    print("\n‚úÖ No changes detected")
    return db


# ================================================================
# MAIN FUNCTION
# ================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Incremental Index Update for RAG ChatBot"
    )
    parser.add_argument(
        "--force", "-f",
        action="store_true",
        help="Force rebuild to√†n b·ªô index"
    )
    parser.add_argument(
        "--dry-run", "-d",
        action="store_true",
        help="Ch·ªâ check changes, kh√¥ng update"
    )
    parser.add_argument(
        "--no-backup",
        action="store_true",
        help="Kh√¥ng backup tr∆∞·ªõc khi update"
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("üîÑ INCREMENTAL INDEX UPDATE")
    print("=" * 60)
    print(f"Data directory: {settings.DATA_DIR}")
    print(f"Vector DB: {settings.DB_DIR}")
    print(f"Mode: {'DRY RUN' if args.dry_run else 'LIVE UPDATE'}")
    print("-" * 60)
    
    # ============ STEP 1: Scan current files ============
    print("\nüìÇ Scanning data directory...")
    
    if not os.path.exists(settings.DATA_DIR):
        print(f"‚ùå Data directory not found: {settings.DATA_DIR}")
        sys.exit(1)
    
    current_files = get_current_files(settings.DATA_DIR)
    print(f"  Found {len(current_files)} files")
    
    # ============ STEP 2: Load manifest ============
    print("\nüìã Loading manifest...")
    manifest = load_manifest()
    
    if manifest.get("last_update"):
        print(f"  Last update: {manifest['last_update']}")
    else:
        print("  No previous manifest found")
    
    # ============ STEP 3: Detect changes ============
    print("\nüîç Detecting changes...")
    
    new_files, modified_files, deleted_files = detect_changes(
        current_files, manifest
    )
    
    print(f"  New files: {len(new_files)}")
    for f in new_files:
        print(f"    + {f}")
    
    print(f"  Modified files: {len(modified_files)}")
    for f in modified_files:
        print(f"    ~ {f}")
    
    print(f"  Deleted files: {len(deleted_files)}")
    for f in deleted_files:
        print(f"    - {f}")
    
    # ============ STEP 4: Check if update needed ============
    total_changes = len(new_files) + len(modified_files) + len(deleted_files)
    
    if not args.force and total_changes == 0:
        print("\n‚úÖ No changes detected. Index is up to date!")
        return
    
    if args.dry_run:
        print("\nüîç DRY RUN - No changes will be made")
        print(f"   Would update: {total_changes} files")
        return
    
    # ============ STEP 5: Backup ============
    if not args.no_backup and os.path.exists(settings.DB_DIR):
        print("\nüíæ Backing up current index...")
        backup_index()
    
    # ============ STEP 6: Initialize embeddings ============
    print("\nüß† Loading embedding model...")
    embeddings = LocalEmbedding()
    
    # ============ STEP 7: Update index ============
    try:
        if args.force or not os.path.exists(settings.DB_DIR):
            # Force rebuild ho·∫∑c ch∆∞a c√≥ index
            db = rebuild_full_index(settings.DATA_DIR, embeddings)
        else:
            # Load existing index v√† update
            print("\nüìÇ Loading existing index...")
            db = FAISS.load_local(
                settings.DB_DIR,
                embeddings,
                allow_dangerous_deserialization=True
            )
            
            db = incremental_update(
                db,
                new_files,
                modified_files,
                deleted_files,
                settings.DATA_DIR,
                embeddings
            )
        
        # ============ STEP 8: Save index ============
        print("\nüíæ Saving updated index...")
        os.makedirs(settings.DB_DIR, exist_ok=True)
        db.save_local(settings.DB_DIR)
        print(f"  ‚úì Saved to: {settings.DB_DIR}")
        
        # ============ STEP 9: Update manifest ============
        print("\nüìã Updating manifest...")
        manifest["files"] = current_files
        save_manifest(manifest)
        print(f"  ‚úì Saved manifest")
        
        # ============ SUMMARY ============
        print("\n" + "=" * 60)
        print("‚úÖ INDEX UPDATE COMPLETED!")
        print("=" * 60)
        print(f"  Total files: {len(current_files)}")
        print(f"  Changes: {total_changes}")
        print(f"  Timestamp: {datetime.now().isoformat()}")
        
    except Exception as e:
        print(f"\n‚ùå ERROR: {str(e)}")
        print("\nüîÑ Attempting rollback...")
        
        # T√¨m backup g·∫ßn nh·∫•t
        if os.path.exists(BACKUP_DIR):
            backups = sorted(os.listdir(BACKUP_DIR), reverse=True)
            if backups:
                restore_from_backup(os.path.join(BACKUP_DIR, backups[0]))
                print("  ‚úì Rollback completed")
            else:
                print("  ‚ö†Ô∏è No backup available")
        
        sys.exit(1)


# ================================================================
# UTILITY COMMANDS
# ================================================================

def list_backups():
    """Li·ªát k√™ c√°c backups"""
    if not os.path.exists(BACKUP_DIR):
        print("No backups found")
        return
    
    backups = sorted(os.listdir(BACKUP_DIR), reverse=True)
    print(f"Found {len(backups)} backups:")
    for backup in backups:
        path = os.path.join(BACKUP_DIR, backup)
        size = sum(
            os.path.getsize(os.path.join(path, f))
            for f in os.listdir(path)
        ) / 1024  # KB
        print(f"  - {backup} ({size:.1f} KB)")


def clean_backups(keep: int = 5):
    """X√≥a backups c≈©, gi·ªØ l·∫°i N b·∫£n g·∫ßn nh·∫•t"""
    if not os.path.exists(BACKUP_DIR):
        return
    
    backups = sorted(os.listdir(BACKUP_DIR), reverse=True)
    
    if len(backups) <= keep:
        print(f"Only {len(backups)} backups, keeping all")
        return
    
    to_delete = backups[keep:]
    for backup in to_delete:
        path = os.path.join(BACKUP_DIR, backup)
        shutil.rmtree(path)
        print(f"  Deleted: {backup}")
    
    print(f"Cleaned {len(to_delete)} old backups")


if __name__ == "__main__":
    main()